# -*- coding: utf-8 -*-
"""Updated_Streamflow.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19feaHnpduE0kyaeeMhQ4D4teeidNxkWZ
"""

# Commented out IPython magic to ensure Python compatibility.
# Importing lIbraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
import tensorflow as tf
import warnings
warnings.filterwarnings('ignore')
from google.colab import drive
drive.mount("/content/drive")
import sys
sys.path.append('/content/drive/My Drive/')
import utils
import plot_utils
import nse_utils
import standardization

# File paths
plvo_file = ("/content/drive/MyDrive/Datasets/raw/raw/PLVO2_USGS.txt")
plvo_file_sim = ("/content/drive/MyDrive/Datasets/raw/raw/PLVO2_sim.txt")

# Read and preprocess the USGS data
df_plvo = utils.read_usgs_data(plvo_file, header_line=32)

# Read and preprocess simulation data
df_plvo_sim = utils.read_simulation_data(plvo_file_sim, header_line=6)

# Merge df_plvo and df_plvo_sim
df_plvo_merge = pd.merge(df_plvo, df_plvo_sim, on='datetime', how='inner')

# Replace "CST" with "UTC" in the 'timezone' column

df_plvo_merge['timezone'] = df_plvo_merge['timezone'].replace('CST', 'UTC')

# Display the modified DataFrame
print(df_plvo_merge.head())

"""**Handling Missing Values**"""

# Check for missing values in df_plvo_merged
missing_values_plvo = df_plvo_merge.isnull().sum()

# Print the number of missing values for each station

print("\nNumber of missing values in df_plvo_merged:")
print(missing_values_plvo)

"""**Filter Date Range**"""

# Convert 'datetime' column to datetime-like object
df_plvo_merge['datetime'] = pd.to_datetime(df_plvo_merge['datetime'])

# Filter the DataFrame for datetime values on "2007-01-01"
filtered_df = df_plvo_merge[df_plvo_merge['datetime'].dt.date == pd.to_datetime("2007-01-01")]

# Display the filtered DataFrame
print(filtered_df)

"""**Timeseries Plot**"""

from plot_utils import plot_observed_vs_simulated_time_series

# Plot observed vs simulated discharge time series for Pauls Valley station
plot_observed_vs_simulated_time_series(df_plvo_merge, df_plvo_merge, 'Pauls Valley')

"""**BenchMarking the Existing Physics Based Model**"""

# Calculate NSE for Antlers station
nse_Pauls_Valley = nse_utils.calculate_nse_for_station(df_plvo_merge, df_plvo_merge)


print("NSE for Pauls Valley station:", nse_Pauls_Valley)

"""**BenchMarking the Existing Physics Based Model**
Based on the Validation time period


---


"""

# Define the start and end dates
start_date = '2016-01-01 00:00:00+00:00'
end_date = '2018-12-31 18:00:00+00:00'

# Create a new DataFrame within the specified timeframe
df_plvo_merge_Val = df_plvo_merge[(df_plvo_merge['datetime'] >= start_date) & (df_plvo_merge['datetime'] <= end_date)]

df_plvo_merge_Val

# Calculate NSE for Antlers station
nse_Pauls_Valley = nse_utils.calculate_nse_for_station(df_plvo_merge_Val, df_plvo_merge_Val)


print("NSE for the validation period at Pauls Valley station:", nse_Pauls_Valley)

df_plvo_merge

df_plvo_merge.head()

"""**Scaling - Standardization**"""

# Standardize the discharge columns
df_plvo_merge = standardization.standardize_discharge_columns(df_plvo_merge, ['Sim_Discharge', 'USGS_Discharge'])

# Display the updated DataFrame
df_plvo_merge.head()

"""**Splitting the Data**"""

df_plvo_merge['datetime'] = pd.to_datetime(df_plvo_merge['datetime'])

train_start, train_end = '2007-01-01 06:00:00+00:00', '2015-12-31 18:00:00+00:00'
val_start, val_end = '2016-01-01 00:00:00+00:00', '2018-12-31 18:00:00+00:00'
test_start, test_end = '2019-01-01 00:00:00+00:00', '2020-12-31 12:00:00+00:00'

train_start = pd.Timestamp(train_start)
train_end = pd.Timestamp(train_end)
val_start = pd.Timestamp(val_start)
val_end = pd.Timestamp(val_end)
test_start = pd.Timestamp(test_start)
test_end = pd.Timestamp(test_end)

df_plvo_merge['datetime'] = pd.to_datetime(df_plvo_merge['datetime'])
df_plvo_train = df_plvo_merge[(df_plvo_merge['datetime'] >= train_start) & (df_plvo_merge['datetime'] <= train_end)]
df_plvo_val = df_plvo_merge[(df_plvo_merge['datetime'] >= val_start) & (df_plvo_merge['datetime'] <= val_end)]
df_plvo_test = df_plvo_merge[(df_plvo_merge['datetime'] >= test_start) & (df_plvo_merge['datetime'] <= test_end)]

df_plvo_train.head()

# Verify lengths of each set
print("Training data length:", len(df_plvo_train))
print("Validation data length:", len(df_plvo_val))
print("Testing data length:", len(df_plvo_test))

df_plvo_train.head()

df_plvo_merge.shape

# Drop columns except 'Sim_Discharge_Standardized' and 'USGS_Discharge'
df_plvo_merged = df_plvo_merge[['Sim_Discharge_Standardized', 'USGS_Discharge_Standardized']]

# Display the first few rows of the modified DataFrame
df_plvo_merged.head()

"""**Feature Engineering**"""

# Define a function to prepare the dataset for LSTM
def prepare_dataset(X_data, y_data, n_steps):
    df_as_np = X_data.to_numpy()
    df2_as_np = y_data.to_numpy()
    X, y = [], []
    for i in range(len(X_data) - n_steps):
        row = [[a] for a in df_as_np[i:i+n_steps]]
        X.append(row)
        label = df2_as_np[i+n_steps]
        y.append(label)
    return np.array(X), np.array(y)
n_steps = 60

# Preparing the dataset for training
X_train, y_train = prepare_dataset(df_plvo_train['Sim_Discharge_Standardized'], df_plvo_train['USGS_Discharge_Standardized'], n_steps)

# Preparing the dataset for validation
X_val, y_val = prepare_dataset(df_plvo_val['Sim_Discharge_Standardized'], df_plvo_val['USGS_Discharge_Standardized'], n_steps)

# Preparing the dataset for testing
X_test, y_test = prepare_dataset(df_plvo_test['Sim_Discharge_Standardized'], df_plvo_test['USGS_Discharge_Standardized'], n_steps)

X_train.shape, y_train.shape,X_val.shape, y_val.shape,X_test.shape, y_test.shape

print(X_train.shape)

"""**Long Short Term Memory**"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import *
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping
from tensorflow.keras.losses import MeanSquaredError
from tensorflow.keras.metrics import RootMeanSquaredError
from tensorflow.keras.optimizers import Adam

model1 = Sequential()
model1.add(InputLayer(input_shape=(60, 1)))
model1.add(LSTM(16, activation='relu', return_sequences=True, kernel_regularizer=tf.keras.regularizers.l2(0.0001)))
model1.add(Dropout(0.35))
model1.add(LSTM(8, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.0001)))
model1.add(Dropout(0.4))
model1.add(Dense(8, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.0001)))
model1.add(Dropout(0.35))
model1.add(Dense(1, activation='linear'))

model1.summary()

model1.compile(optimizer=Adam(learning_rate=0.001), loss=MeanSquaredError(), metrics=[RootMeanSquaredError()])

early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

history = model1.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=50, batch_size=16, callbacks=[early_stopping])

"""**Performance**"""

# Plot training history
plt.figure(figsize=(10, 5))

# Plot training loss
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

# Plot training RMSE
plt.subplot(1, 2, 2)
plt.plot(history.history['root_mean_squared_error'], label='Training RMSE')
plt.plot(history.history['val_root_mean_squared_error'], label='Validation RMSE')
plt.title('Training and Validation RMSE')
plt.xlabel('Epoch')
plt.ylabel('RMSE')
plt.legend()

plt.tight_layout()
plt.show()

X_val.shape

y_val.shape

# Since the X_val and y_val are the validation data
predictions = model1.predict(X_val)

#Create a DataFrame with predictions and actual values
df_result = pd.DataFrame({'Predictions': predictions.flatten(), 'Actual': y_val.flatten()})

# Print or use the DataFrame as needed
print(df_result)

"""**Calculating the NSE For LSTM**"""

y_mean = np.mean(y_val)  # Calculate the mean of actual target values

# Calculate the sum of squares of residuals
residuals = y_val - predictions.flatten()
SS_residuals = np.sum(residuals ** 2)

# Calculate the sum of squares of total
SS_total = np.sum((y_val - y_mean) ** 2)

# Calculate NSE
NSE = 1 - (SS_residuals / SS_total)

print("NSE value:", NSE)

from sklearn.metrics import mean_squared_error

# Assuming y_test is your true target values
mse = mean_squared_error(y_val, predictions)
print("Mean Squared Error:", mse)

"""**Bi-directional LSTM**"""